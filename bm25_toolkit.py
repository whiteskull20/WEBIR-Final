#!/usr/bin/env python3
"""
BM25 Music Retrieval Toolkit

ä½¿ç”¨ç¯„ä¾‹:
    from bm25_toolkit import BM25Retriever
    
    retriever = BM25Retriever('ppr4env_index')
    results = retriever.search('query.mid', limit=10)
"""

import math
import pickle
import json
import numpy as np
from pathlib import Path
from typing import List, Tuple, Dict, Optional
from collections import defaultdict

# å°å…¥éŸ³æ¨‚è™•ç†ç›¸é—œæ¨¡çµ„
try:
    from midi_parser import MIDIEventExtractor
    from ppr4env_music_retrieval import MusicNGramExtractor
except ImportError as e:
    print(f"è­¦å‘Š: ç„¡æ³•å°å…¥éŸ³æ¨‚è™•ç†æ¨¡çµ„ï¼Œè«‹ç¢ºä¿ midi_parser.py å’Œ ppr4env_music_retrieval.py åœ¨è·¯å¾‘ä¸­: {e}")


class BM25Retriever:
    """
    BM25éŸ³æ¨‚æª¢ç´¢å™¨
    å°ˆé–€ç”¨æ–¼PPR4ENVç´¢å¼•çš„BM25æª¢ç´¢
    """
    
    def __init__(self, index_dir: str, k1: float = 1.2, b: float = 0.75):
        """
        åˆå§‹åŒ–BM25æª¢ç´¢å™¨
        
        Args:
            index_dir: PPR4ENVç´¢å¼•ç›®éŒ„è·¯å¾‘
            k1: BM25åƒæ•¸ï¼Œæ§åˆ¶è©é »é£½å’Œé» (é è¨­1.2)
            b: BM25åƒæ•¸ï¼Œæ§åˆ¶æ–‡æª”é•·åº¦æ­£è¦åŒ– (é è¨­0.75)
        """
        self.index_dir = Path(index_dir)
        self.k1 = k1
        self.b = b
        
        # ç´¢å¼•æ•¸æ“šçµæ§‹
        self.word_pool = {}          # word -> word_id
        self.id_to_word = {}         # word_id -> word
        self.doc_pool = {}           # doc_id_str -> doc_id_int
        self.id_to_doc = {}          # doc_id_int -> doc_id_str
        self.inverted_index = {}     # word_id -> [(doc_id_int, positions_array), ...]
        self.vocabulary = {}         # word_id -> {'doc_freq': int, 'total_freq': int}
        self.document_info = {}      # doc_id_int -> {'word_count': int, ...}
        
        # BM25è¨ˆç®—æ‰€éœ€çµ±è¨ˆ
        self.total_docs = 0
        self.avg_doc_length = 0.0
        self.doc_lengths = {}        # doc_id_int -> length
        self.idf_cache = {}          # word_id -> idf_value
        
        # éŸ³æ¨‚è™•ç†å·¥å…·
        self.midi_extractor = MIDIEventExtractor() if 'MIDIEventExtractor' in globals() else None
        self.ngram_extractor = MusicNGramExtractor() if 'MusicNGramExtractor' in globals() else None
        
        # è¼‰å…¥ç´¢å¼•
        self._load_index()
        self._precompute_bm25_stats()
    
    def _load_index(self):
        """è¼‰å…¥PPR4ENVç´¢å¼•æ–‡ä»¶"""
        required_files = [
            'word_mappings.pkl',
            'doc_mappings.pkl', 
            'inverted_index.pkl',
            'vocabulary.pkl',
            'documents.pkl',
        ]
        
        # æª¢æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for filename in required_files:
            if not (self.index_dir / filename).exists():
                raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_dir / filename}")
        
        try:
            # è¼‰å…¥è©å½™æ˜ å°„
            print("è¼‰å…¥word_mappings.pkl")
            with open(self.index_dir / 'word_mappings.pkl', 'rb') as f:
                word_data = pickle.load(f)
                self.word_pool = word_data['word_pool']
                self.id_to_word = word_data['id_to_word']
            
            print("è¼‰å…¥doc_mappings.pkl")
            # è¼‰å…¥æ–‡æª”æ˜ å°„
            with open(self.index_dir / 'doc_mappings.pkl', 'rb') as f:
                doc_data = pickle.load(f)
                self.doc_pool = doc_data['doc_pool']
                self.id_to_doc = doc_data['id_to_doc']
            
            print("è¼‰å…¥inverted_index.pkl")
            # è¼‰å…¥å€’æ’ç´¢å¼•
            with open(self.index_dir / 'inverted_index.pkl', 'rb') as f:
                self.inverted_index = pickle.load(f)
            
            print("è¼‰å…¥vocabulary.pkl")
            # è¼‰å…¥è©å½™è¡¨
            with open(self.index_dir / 'vocabulary.pkl', 'rb') as f:
                self.vocabulary = pickle.load(f)
            
            print("è¼‰å…¥documents.pkl")
            # è¼‰å…¥æ–‡æª”ä¿¡æ¯
            with open(self.index_dir / 'documents.pkl', 'rb') as f:
                self.document_info = pickle.load(f)
    
            
            print(f"âœ… BM25ç´¢å¼•è¼‰å…¥æˆåŠŸ:")
            print(f"   ğŸ“ æ–‡æª”æ•¸: {len(self.document_info):,}")
            print(f"   ğŸ“ è©å½™æ•¸: {len(self.vocabulary):,}")
            
        except Exception as e:
            raise RuntimeError(f"è¼‰å…¥ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _precompute_bm25_stats(self):
        """é è¨ˆç®—BM25æ‰€éœ€çµ±è¨ˆä¿¡æ¯"""
        print("ğŸ”„ é è¨ˆç®—BM25çµ±è¨ˆä¿¡æ¯...")
        
        # è¨ˆç®—æ–‡æª”é•·åº¦å’Œç¸½æ–‡æª”æ•¸
        self.total_docs = len(self.document_info)
        total_length = 0
        
        for doc_id_int, doc_info in self.document_info.items():
            doc_length = doc_info.get('word_count', 0)
            self.doc_lengths[doc_id_int] = doc_length
            total_length += doc_length
        
        # è¨ˆç®—å¹³å‡æ–‡æª”é•·åº¦
        self.avg_doc_length = total_length / self.total_docs if self.total_docs > 0 else 0
        
        # é è¨ˆç®—æ‰€æœ‰è©å½™çš„IDFå€¼
        for word_id, vocab_info in self.vocabulary.items():
            doc_freq = vocab_info.get('doc_freq', 0)
            if doc_freq > 0:
                # BM25 IDFå…¬å¼: log((N - df + 0.5) / (df + 0.5))
                idf = math.log((self.total_docs - doc_freq + 0.5) / (doc_freq + 0.5))
                self.idf_cache[word_id] = max(0.0, idf)  # ç¢ºä¿IDFéè² 
            else:
                self.idf_cache[word_id] = 0.0
        
        print(f"âœ… BM25çµ±è¨ˆå®Œæˆ:")
        print(f"   ğŸ“Š ç¸½æ–‡æª”æ•¸: {self.total_docs:,}")
        print(f"   ğŸ“ å¹³å‡æ–‡æª”é•·åº¦: {self.avg_doc_length:.2f}")
        print(f"   ğŸ”¤ IDFå€¼ç¯„åœ: {min(self.idf_cache.values()):.3f} - {max(self.idf_cache.values()):.3f}")
    
    def search(self, query_midi_file: str, limit: int = 10) -> List[Tuple[str, float, Dict]]:
        """
        å¾MIDIæ–‡ä»¶åŸ·è¡ŒBM25æœå°‹
        
        Args:
            query_midi_file: æŸ¥è©¢MIDIæ–‡ä»¶è·¯å¾‘
            limit: è¿”å›çµæœæ•¸é‡é™åˆ¶
            
        Returns:
            List[Tuple[str, float, Dict]]: [(doc_id, bm25_score, details), ...]
        """
        if not self.midi_extractor or not self.ngram_extractor:
            raise RuntimeError("éŸ³æ¨‚è™•ç†æ¨¡çµ„æœªæ­£ç¢ºè¼‰å…¥ï¼Œè«‹æª¢æŸ¥ä¾è³´")
        
        print(f"ğŸµ è™•ç†æŸ¥è©¢æ–‡ä»¶: {query_midi_file}")
        
        # 1. æå–MIDIäº‹ä»¶
        events = self.midi_extractor.extract_events_from_file(query_midi_file)
        if not events:
            raise ValueError(f"ç„¡æ³•å¾ {query_midi_file} æå–éŸ³æ¨‚äº‹ä»¶")
        
        # 2. åˆ†çµ„äº‹ä»¶
        grouped_events = self._group_events_by_onset(events)
        if not grouped_events:
            raise ValueError("ç„¡æ³•åˆ†çµ„éŸ³æ¨‚äº‹ä»¶")
        
        # 3. æå–N-grams
        words_with_positions = self.ngram_extractor.extract_ngrams_with_positions(grouped_events)
        if not words_with_positions:
            raise ValueError("ç„¡æ³•ç”ŸæˆéŸ³æ¨‚è©å½™")
        
        query_words = [word for word, _ in words_with_positions]
        
        #print(f"ğŸ“ ç”ŸæˆæŸ¥è©¢è©å½™: {len(query_words)} å€‹")
        #print(f"   å‰10å€‹: {query_words[:10]}")
        
        # 4. åŸ·è¡ŒBM25æœå°‹
        return self.search_words(query_words, limit)
    
    def search_words(self, query_words: List[str], limit: int = 10) -> List[Tuple[str, float, Dict]]:
        """
        ä½¿ç”¨éŸ³æ¨‚è©å½™åŸ·è¡ŒBM25æœå°‹
        
        Args:
            query_words: æŸ¥è©¢è©å½™åˆ—è¡¨
            limit: è¿”å›çµæœæ•¸é‡é™åˆ¶
            
        Returns:
            List[Tuple[str, float, Dict]]: [(doc_id, bm25_score, details), ...]
        """
        if not query_words:
            return []
        
        #print(f"ğŸ” åŸ·è¡ŒBM25æœå°‹ï¼ŒæŸ¥è©¢è©å½™: {len(query_words)} å€‹")
        
        # è½‰æ›æŸ¥è©¢è©ç‚ºword_id
        query_word_ids = []
        query_word_mapping = {}  # word_id -> original_word
        
        for word in query_words:
            if word in self.word_pool:
                word_id = self.word_pool[word]
                query_word_ids.append(word_id)
                query_word_mapping[word_id] = word
        
        if not query_word_ids:
            print("âŒ æŸ¥è©¢è©å½™åœ¨ç´¢å¼•ä¸­æœªæ‰¾åˆ°")
            return []
        
        #print(f"ğŸ“Š æœ‰æ•ˆæŸ¥è©¢è©å½™: {len(query_word_ids)}/{len(query_words)}")
        
        # ç²å–å€™é¸æ–‡æª”
        candidate_docs = self._get_candidate_documents(query_word_ids)
        #print(f"ğŸ¯ å€™é¸æ–‡æª”æ•¸: {len(candidate_docs)}")
        
        if not candidate_docs:
            return []
        
        # è¨ˆç®—BM25åˆ†æ•¸
        doc_scores = defaultdict(float)
        doc_details = defaultdict(lambda: {
            'matched_terms': {},
            'doc_length': 0,
            'query_terms_found': 0,
            'total_query_terms': len(query_word_ids),
            'coverage': 0.0
        })
        
        # å°æ¯å€‹æŸ¥è©¢è©è¨ˆç®—å…¶å°æ‰€æœ‰å€™é¸æ–‡æª”çš„è²¢ç»
        for word_id in query_word_ids:
            if word_id not in self.inverted_index:
                continue
            
            word = query_word_mapping[word_id]
            idf = self.idf_cache.get(word_id, 0.0)
            
            # éæ­·åŒ…å«æ­¤è©çš„æ‰€æœ‰æ–‡æª”
            for doc_id_int, positions_array in self.inverted_index[word_id]:
                if doc_id_int not in candidate_docs:
                    continue
                
                # è¨ˆç®—è©é »(TF)
                tf = len(positions_array)
                
                # ç²å–æ–‡æª”é•·åº¦
                doc_length = self.doc_lengths.get(doc_id_int, 1)
                
                # BM25åˆ†æ•¸è¨ˆç®—
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)
                term_score = idf * (numerator / denominator)
                
                # ç´¯åŠ åˆ†æ•¸
                doc_scores[doc_id_int] += term_score
                
                # è¨˜éŒ„è©³ç´°ä¿¡æ¯
                doc_details[doc_id_int]['matched_terms'][word] = {
                    'tf': tf,
                    'idf': idf,
                    'term_score': term_score,
                    'positions_count': len(positions_array)
                }
                doc_details[doc_id_int]['doc_length'] = doc_length
                doc_details[doc_id_int]['query_terms_found'] += 1
        
        # è¨ˆç®—è¦†è“‹ç‡
        for doc_id_int in doc_details:
            found = doc_details[doc_id_int]['query_terms_found']
            total = doc_details[doc_id_int]['total_query_terms']
            doc_details[doc_id_int]['coverage'] = found / total if total > 0 else 0.0
        
        # æ’åºçµæœ
        sorted_results = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        # æ ¼å¼åŒ–è¼¸å‡º
        results = []
        for doc_id_int, score in sorted_results[:limit]:
            doc_id_str = self.id_to_doc[doc_id_int]
            details = dict(doc_details[doc_id_int])
            results.append((doc_id_str, score, details))
        
        #print(f"âœ… æ‰¾åˆ° {len(results)} å€‹çµæœ")
        return results
    
    def _get_candidate_documents(self, query_word_ids: List[int]) -> set:
        """ç²å–åŒ…å«ä»»æ„æŸ¥è©¢è©çš„å€™é¸æ–‡æª”"""
        candidate_docs = set()
        for word_id in query_word_ids:
            if word_id in self.inverted_index:
                for doc_id_int, _ in self.inverted_index[word_id]:
                    candidate_docs.add(doc_id_int)
        return candidate_docs
    
    def _group_events_by_onset(self, events: List[Tuple[int, int]]) -> List[Tuple[int, List[int]]]:
        """å°‡éŸ³æ¨‚äº‹ä»¶æŒ‰onsetæ™‚é–“åˆ†çµ„"""
        if not events:
            return []
        
        grouped = defaultdict(list)
        for onset_time, pitch in events:
            grouped[onset_time].append(pitch)
        
        # è½‰æ›ç‚ºæ’åºåˆ—è¡¨
        result = []
        for onset_time in sorted(grouped.keys()):
            pitches = sorted(grouped[onset_time])
            result.append((onset_time, pitches))
        
        return result
    
    def explain_score(self, doc_id: str, query_words: List[str]) -> Dict:
        """
        è§£é‡‹ç‰¹å®šæ–‡æª”çš„BM25åˆ†æ•¸è¨ˆç®—
        
        Args:
            doc_id: æ–‡æª”ID
            query_words: æŸ¥è©¢è©å½™åˆ—è¡¨
            
        Returns:
            Dict: è©³ç´°çš„åˆ†æ•¸è§£é‡‹
        """
        if doc_id not in self.doc_pool:
            return {"error": f"æ–‡æª”ä¸å­˜åœ¨: {doc_id}"}
        
        doc_id_int = self.doc_pool[doc_id]
        doc_length = self.doc_lengths.get(doc_id_int, 0)
        
        explanation = {
            'doc_id': doc_id,
            'doc_length': doc_length,
            'avg_doc_length': self.avg_doc_length,
            'total_docs': self.total_docs,
            'bm25_params': {'k1': self.k1, 'b': self.b},
            'term_explanations': [],
            'total_score': 0.0
        }
        
        total_score = 0.0
        
        for word in query_words:
            if word not in self.word_pool:
                continue
            
            word_id = self.word_pool[word]
            
            # æª¢æŸ¥è©æ˜¯å¦åœ¨æ–‡æª”ä¸­
            tf = 0
            positions = []
            
            if word_id in self.inverted_index:
                for d_id, pos_array in self.inverted_index[word_id]:
                    if d_id == doc_id_int:
                        tf = len(pos_array)
                        positions = pos_array.tolist() if hasattr(pos_array, 'tolist') else list(pos_array)
                        break
            
            # è¨ˆç®—åˆ†æ•¸çµ„ä»¶
            idf = self.idf_cache.get(word_id, 0.0)
            doc_freq = self.vocabulary.get(word_id, {}).get('doc_freq', 0)
            
            if tf > 0:
                # BM25çµ„ä»¶è¨ˆç®—
                numerator = tf * (self.k1 + 1)
                length_norm = 1 - self.b + self.b * doc_length / self.avg_doc_length
                denominator = tf + self.k1 * length_norm
                term_score = idf * (numerator / denominator)
                total_score += term_score
            else:
                term_score = 0.0
            
            term_explanation = {
                'term': word,
                'tf': tf,
                'doc_freq': doc_freq,
                'idf': idf,
                'idf_calculation': f"log(({self.total_docs} - {doc_freq} + 0.5) / ({doc_freq} + 0.5))",
                'bm25_numerator': tf * (self.k1 + 1) if tf > 0 else 0,
                'length_normalization': 1 - self.b + self.b * doc_length / self.avg_doc_length,
                'bm25_denominator': tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length) if tf > 0 else 0,
                'term_score': term_score,
                'positions': positions[:20] if len(positions) <= 20 else positions[:20] + ['...']  # æœ€å¤šé¡¯ç¤º20å€‹ä½ç½®
            }
            
            explanation['term_explanations'].append(term_explanation)
        
        explanation['total_score'] = total_score
        return explanation
    
    def get_doc_info(self, doc_id: str) -> Dict:
        """
        ç²å–æ–‡æª”è©³ç´°ä¿¡æ¯
        
        Args:
            doc_id: æ–‡æª”ID
            
        Returns:
            Dict: æ–‡æª”ä¿¡æ¯
        """
        if doc_id not in self.doc_pool:
            return {"error": f"æ–‡æª”ä¸å­˜åœ¨: {doc_id}"}
        
        doc_id_int = self.doc_pool[doc_id]
        doc_info = self.document_info.get(doc_id_int, {})
        
        return {
            'doc_id': doc_id,
            'file_name': doc_info.get('file_name', 'N/A'),
            'file_path': doc_info.get('file_path', 'N/A'),
            'word_count': doc_info.get('word_count', 0),
            'unique_words': doc_info.get('unique_words', 0),
            'track_id': doc_info.get('track_id', 'N/A')
        }
    
    def get_statistics(self) -> Dict:
        """
        ç²å–BM25æª¢ç´¢å™¨çµ±è¨ˆä¿¡æ¯
        
        Returns:
            Dict: çµ±è¨ˆä¿¡æ¯
        """
        idf_values = list(self.idf_cache.values())
        doc_lengths = list(self.doc_lengths.values())
        
        return {
            'index_stats': {
                'total_documents': self.total_docs,
                'total_vocabulary': len(self.vocabulary),
                'avg_doc_length': self.avg_doc_length
            },
            'bm25_params': {
                'k1': self.k1,
                'b': self.b
            },
            'doc_length_stats': {
                'min': min(doc_lengths) if doc_lengths else 0,
                'max': max(doc_lengths) if doc_lengths else 0,
                'mean': np.mean(doc_lengths) if doc_lengths else 0,
                'std': np.std(doc_lengths) if doc_lengths else 0
            },
            'idf_stats': {
                'min': min(idf_values) if idf_values else 0,
                'max': max(idf_values) if idf_values else 0,
                'mean': np.mean(idf_values) if idf_values else 0,
                'std': np.std(idf_values) if idf_values else 0
            }
        }


# å·¥å…·å‡½æ•¸
def display_results(results: List[Tuple[str, float, Dict]], query_description: str = ""):
    """
    å‹å¥½åœ°é¡¯ç¤ºBM25æª¢ç´¢çµæœ
    
    Args:
        results: BM25æª¢ç´¢çµæœ
        query_description: æŸ¥è©¢æè¿°
    """
    print(f"\nğŸ¯ BM25æª¢ç´¢çµæœ {f'({query_description})' if query_description else ''}")
    print(f"æ‰¾åˆ° {len(results)} å€‹çµæœ:")
    print("=" * 80)
    
    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°åŒ¹é…çµæœ")
        return
    
    for i, (doc_id, score, details) in enumerate(results):
        print(f"\n{i+1:2d}. ğŸ“„ æ–‡æª”: {doc_id}")
        print(f"     ğŸ“Š BM25åˆ†æ•¸: {score:.4f}")
        print(f"     ğŸ¯ åŒ¹é…è©å½™: {details['query_terms_found']}/{details['total_query_terms']}")
        print(f"     ğŸ“ˆ è¦†è“‹ç‡: {details['coverage']:.2%}")
        print(f"     ğŸ“ æ–‡æª”é•·åº¦: {details['doc_length']}")
        
        # é¡¯ç¤ºåŒ¹é…çš„è©å½™è©³æƒ…
        if details['matched_terms']:
            matched_terms = list(details['matched_terms'].keys())[:5]  # åªé¡¯ç¤ºå‰5å€‹
            print(f"     ğŸ”¤ åŒ¹é…è©å½™: {', '.join(matched_terms)}")
            if len(details['matched_terms']) > 5:
                print(f"          ... é‚„æœ‰ {len(details['matched_terms']) - 5} å€‹è©å½™")


# ä½¿ç”¨ç¯„ä¾‹
def demo_usage():
    """ç¤ºç¯„å¦‚ä½•ä½¿ç”¨BM25æª¢ç´¢å™¨"""
    print("ğŸ” BM25 Music Retrieval Toolkit ç¤ºç¯„")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–æª¢ç´¢å™¨
        retriever = BM25Retriever('ppr4env_index', k1=1.2, b=0.75)
        
        print("\nğŸ“ ç¤ºç¯„:")
        results = retriever.search(r'.\lmd_matched\A\A\D\TRAADKW128E079503A\72cae5077339f6abaee4cad318b1e923.mid ', limit=20)
        display_results(results, "MIDIæ–‡ä»¶æŸ¥è©¢")
        results = retriever.search(r'.\lmd_matched\A\B\Z\TRABZDG128F931803F\d6371159fa42356ebd38380f7f0feec2.mid', limit=20)
        display_results(results, "MIDIæ–‡ä»¶æŸ¥è©¢")
        results = retriever.search(r'.\lmd_matched\A\B\R\TRABRUJ128F42ADE0D\025fb4d42ff236cb25881dde5c380c1a.mid', limit=20)
        display_results(results, "MIDIæ–‡ä»¶æŸ¥è©¢")
        results = retriever.search(r'.\lmd_matched\A\C\Q\TRACQIB128F147DAC4\313b6893e29a54a5872df5c92199eadf.mid', limit=20)
        display_results(results, "MIDIæ–‡ä»¶æŸ¥è©¢")
        

    except Exception as e:
        print(f"âŒ ç¤ºç¯„åŸ·è¡Œå¤±æ•—: {e}")


if __name__ == "__main__":
    demo_usage()