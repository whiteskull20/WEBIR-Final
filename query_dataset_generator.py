#!/usr/bin/env python3
"""
æŸ¥è©¢æ•¸æ“šé›†ç”Ÿæˆå™¨ - ç”¨æ–¼PPR4ENVéŸ³æ¨‚æª¢ç´¢ç³»çµ±
å¾lmd_matchedè³‡æ–™å¤¾ä¸­ç”ŸæˆæŸ¥è©¢æ•¸æ“šé›†ï¼ŒåŒ…å«GROUND TRUTH
"""

import os
import random
import json
import argparse
import shutil
from pathlib import Path
from collections import defaultdict
import time
import pretty_midi
import numpy as np

from midi_file_scanner import MIDIFileScanner
from midi_parser import MIDIEventExtractor

class QueryDatasetGenerator:
    def __init__(self, lmd_matched_path):
        """
        åˆå§‹åŒ–æŸ¥è©¢æ•¸æ“šé›†ç”Ÿæˆå™¨
        
        Args:
            lmd_matched_path: lmd_matchedè³‡æ–™å¤¾è·¯å¾‘
        """
        self.lmd_matched_path = Path(lmd_matched_path)
        self.scanner = MIDIFileScanner(lmd_matched_path)
        self.extractor = MIDIEventExtractor()
        
    def get_category_range(self, max_category):
        """
        æ ¹æ“šè¼¸å…¥çš„ä¸‰å€‹å­—æ¯ç”Ÿæˆåˆ†é¡ç¯„åœ
        
        Args:
            max_category: ä¸‰å€‹å­—æ¯ï¼Œä¾‹å¦‚ "ABC", "CCC", "ZZZ"
            
        Returns:
            list: æ‰€æœ‰åœ¨ç¯„åœå…§çš„åˆ†é¡è·¯å¾‘
        """
        if len(max_category) != 3:
            raise ValueError("åˆ†é¡å¿…é ˆæ˜¯ä¸‰å€‹å­—æ¯ï¼Œä¾‹å¦‚ï¼šABC, CCC, ZZZ")
        
        max_category = max_category.upper()
        categories = []
        
        # ç”Ÿæˆæ‰€æœ‰å¾AAAåˆ°max_categoryçš„çµ„åˆ
        for first in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
            if first > max_category[0]:
                break
                
            for second in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
                if first == max_category[0] and second > max_category[1]:
                    break
                    
                for third in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
                    if (first == max_category[0] and second == max_category[1] and 
                        third > max_category[2]):
                        break
                    
                    category_path = f"{first}/{second}/{third}"
                    full_path = self.lmd_matched_path / first / second / third
                    
                    # æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨
                    if full_path.exists():
                        categories.append(category_path)
        
        return categories
    
    def scan_files_in_range(self, max_category):
        """
        æƒææŒ‡å®šç¯„åœå…§çš„æ‰€æœ‰MIDIæª”æ¡ˆï¼Œä¸¦æŒ‰TRACKåˆ†çµ„
        
        Args:
            max_category: æœ€å¤§åˆ†é¡ç¯„åœ
            
        Returns:
            dict: {track_id: [(file_path, file_name, category), ...], ...}
        """
        categories = self.get_category_range(max_category)
        files_by_track = defaultdict(list)
        total_files = 0
        
        print(f"æƒæç¯„åœï¼šA/A/A åˆ° {max_category}")
        print(f"æ‰¾åˆ° {len(categories)} å€‹åˆ†é¡")
        
        for category in categories:
            category_path = self.lmd_matched_path / category.replace('/', os.sep)
            
            if not category_path.exists():
                continue
                
            # æƒæè©²åˆ†é¡ä¸‹çš„æ‰€æœ‰TRACKè³‡æ–™å¤¾
            for track_dir in category_path.iterdir():
                if track_dir.is_dir() and track_dir.name.startswith('TR'):
                    track_id = track_dir.name
                    
                    # åœ¨TRACKè³‡æ–™å¤¾ä¸­å°‹æ‰¾æ‰€æœ‰MIDIæª”æ¡ˆ
                    midi_files = list(track_dir.glob("*.mid"))
                    
                    if midi_files:  # åªæœ‰ç•¶æœ‰MIDIæª”æ¡ˆæ™‚æ‰è¨˜éŒ„
                        for midi_file in midi_files:
                            files_by_track[track_id].append((
                                str(midi_file),
                                midi_file.name,
                                category
                            ))
                            total_files += 1
        
        print(f"ç¸½å…±æ‰¾åˆ° {len(files_by_track)} å€‹TRACKï¼Œ{total_files} å€‹MIDIæª”æ¡ˆ")
        
        # é¡¯ç¤ºæ¯å€‹TRACKçš„æª”æ¡ˆæ•¸çµ±è¨ˆ
        track_file_counts = {track: len(files) for track, files in files_by_track.items()}
        multi_file_tracks = {track: count for track, count in track_file_counts.items() if count > 1}
        
        if multi_file_tracks:
            print(f"ç™¼ç¾ {len(multi_file_tracks)} å€‹TRACKæœ‰å¤šå€‹MIDIæª”æ¡ˆï¼š")
            for track, count in sorted(multi_file_tracks.items())[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"  {track}: {count} å€‹æª”æ¡ˆ")
            if len(multi_file_tracks) > 10:
                print(f"  ... é‚„æœ‰ {len(multi_file_tracks) - 10} å€‹")
        
        return files_by_track
    
    def extract_query_segment(self, file_path, max_duration_seconds=10):
        """
        å¾MIDIæª”æ¡ˆä¸­æå–æŸ¥è©¢ç‰‡æ®µ
        
        Args:
            file_path: MIDIæª”æ¡ˆè·¯å¾‘
            max_duration_seconds: æœ€å¤§æ™‚é•·ï¼ˆç§’ï¼‰
            
        Returns:
            dict: æŸ¥è©¢è³‡è¨Šï¼ŒåŒ…å«äº‹ä»¶ã€æ™‚é•·ç­‰
        """
        try:
            # è¼‰å…¥MIDIæª”æ¡ˆ
            midi_data = pretty_midi.PrettyMIDI(file_path)
            
            # ç²å–ç¸½æ™‚é•·
            total_duration = midi_data.get_end_time()
            
            if total_duration <= max_duration_seconds:
                # æª”æ¡ˆæœ¬èº«å°±ä¸é•·ï¼Œç›´æ¥ä½¿ç”¨æ•´å€‹æª”æ¡ˆ
                start_time = 0
                end_time = total_duration
            else:
                # éš¨æ©Ÿé¸æ“‡èµ·å§‹æ™‚é–“
                max_start = total_duration - max_duration_seconds
                start_time = random.uniform(0, max_start)
                end_time = start_time + max_duration_seconds
            
            # æå–æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„äº‹ä»¶
            events = []
            
            for instrument in midi_data.instruments:
                if instrument.is_drum:
                    continue
                    
                for note in instrument.notes:
                    # æª¢æŸ¥éŸ³ç¬¦æ˜¯å¦åœ¨æ™‚é–“ç¯„åœå…§
                    if (note.start >= start_time and note.start < end_time):
                        # è½‰æ›ç‚ºç›¸å°æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰
                        relative_onset = int((note.start - start_time) * 1000)
                        pitch = int(note.pitch)
                        events.append((relative_onset, pitch))
            
            if not events:
                return None
            
            # æŒ‰æ™‚é–“æ’åº
            events.sort(key=lambda x: x[0])
            
            return {
                'events': events,
                'duration_seconds': end_time - start_time,
                'start_time': start_time,
                'end_time': end_time,
                'original_duration': total_duration,
                'num_events': len(events)
            }
            
        except Exception as e:
            print(f"è™•ç†æª”æ¡ˆ {file_path} æ™‚å‡ºéŒ¯: {e}")
            return None
    
    def create_query_midi(self, events, output_path, tempo=120):
        """
        å°‡äº‹ä»¶è½‰æ›ç‚ºæŸ¥è©¢MIDIæª”æ¡ˆ
        
        Args:
            events: [(onset_time_ms, pitch), ...]
            output_path: è¼¸å‡ºMIDIæª”æ¡ˆè·¯å¾‘
            tempo: ç¯€æ‹é€Ÿåº¦
        """
        # å‰µå»ºæ–°çš„MIDIç‰©ä»¶
        midi_obj = pretty_midi.PrettyMIDI(initial_tempo=tempo)
        
        # å‰µå»ºæ¨‚å™¨
        instrument = pretty_midi.Instrument(program=0)  # Piano
        
        # å°‡äº‹ä»¶è½‰æ›ç‚ºéŸ³ç¬¦
        for i, (onset_time_ms, pitch) in enumerate(events):
            start_time = onset_time_ms / 1000.0  # è½‰æ›ç‚ºç§’
            
            # æ±ºå®šéŸ³ç¬¦é•·åº¦ï¼ˆç°¡å–®ç­–ç•¥ï¼šåˆ°ä¸‹ä¸€å€‹éŸ³ç¬¦æˆ–é»˜èªé•·åº¦ï¼‰
            if i + 1 < len(events):
                next_onset = events[i + 1][0] / 1000.0
                duration = min(next_onset - start_time, 0.5)  # æœ€é•·0.5ç§’
            else:
                duration = 0.5  # æœ€å¾Œä¸€å€‹éŸ³ç¬¦é»˜èª0.5ç§’
            
            duration = max(duration, 0.1)  # æœ€çŸ­0.1ç§’
            
            note = pretty_midi.Note(
                velocity=80,
                pitch=pitch,
                start=start_time,
                end=start_time + duration
            )
            instrument.notes.append(note)
        
        midi_obj.instruments.append(instrument)
        midi_obj.write(str(output_path))
    
    def build_ground_truth(self, files_by_track, query_track_info):
        """
        å»ºç«‹GROUND TRUTHè³‡æ–™çµæ§‹
        
        Args:
            files_by_track: æ‰€æœ‰æª”æ¡ˆæŒ‰TRACKåˆ†çµ„çš„å­—å…¸
            query_track_info: æŸ¥è©¢ä¾†æºçš„TRACKè³‡è¨Š
            
        Returns:
            dict: GROUND TRUTHçµæ§‹
        """
        source_track = query_track_info['track_id']
        source_file = query_track_info['file_path']
        
        # GROUND TRUTHåŒ…å«ï¼š
        # 1. ä¸»è¦ç­”æ¡ˆï¼šä¾†æºæª”æ¡ˆæœ¬èº«
        # 2. ç›¸é—œç­”æ¡ˆï¼šåŒä¸€TRACKä¸‹çš„å…¶ä»–æª”æ¡ˆï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        
        ground_truth = {
            'primary_answer': {
                'track_id': source_track,
                'file_path': source_file,
                'file_name': query_track_info['file_name'],
                'category': query_track_info['category'],
                'relevance_score': 1.0,  # æœ€é«˜ç›¸é—œæ€§
                'note': 'Source file - exact match'
            },
            'related_answers': [],
            'total_relevant_docs': 1  # è‡³å°‘æœ‰ä¾†æºæª”æ¡ˆ
        }
        
        # æ·»åŠ åŒä¸€TRACKä¸‹çš„å…¶ä»–æª”æ¡ˆä½œç‚ºç›¸é—œç­”æ¡ˆ
        if source_track in files_by_track:
            for file_path, file_name, category in files_by_track[source_track]:
                if file_path != source_file:  # æ’é™¤ä¾†æºæª”æ¡ˆæœ¬èº«
                    ground_truth['related_answers'].append({
                        'track_id': source_track,
                        'file_path': file_path,
                        'file_name': file_name,
                        'category': category,
                        'relevance_score': 0.8,  # é«˜ç›¸é—œæ€§ï¼ˆåŒä¸€TRACKï¼‰
                        'note': 'Same track - likely related'
                    })
                    ground_truth['total_relevant_docs'] += 1
        
        return ground_truth
    
    def generate_dataset(self, max_category="CCC", num_queries=100, 
                        max_duration_seconds=10, output_dir="query_dataset"):
        """
        ç”ŸæˆæŸ¥è©¢æ•¸æ“šé›†ï¼ŒåŒ…å«GROUND TRUTH
        
        Args:
            max_category: æœ€å¤§åˆ†é¡ç¯„åœï¼ˆä¸‰å€‹å­—æ¯ï¼‰
            num_queries: è¦ç”Ÿæˆçš„æŸ¥è©¢æ•¸é‡
            max_duration_seconds: æ¯å€‹æŸ¥è©¢çš„æœ€å¤§æ™‚é•·
            output_dir: è¼¸å‡ºç›®éŒ„
        """
        print(f"é–‹å§‹ç”ŸæˆæŸ¥è©¢æ•¸æ“šé›†...")
        print(f"åƒæ•¸è¨­å®šï¼š")
        print(f"  - åˆ†é¡ç¯„åœï¼šA/A/A åˆ° {max_category}")
        print(f"  - æŸ¥è©¢æ•¸é‡ï¼š{num_queries}")
        print(f"  - æœ€å¤§æ™‚é•·ï¼š{max_duration_seconds} ç§’")
        print(f"  - è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # å‰µå»ºå­ç›®éŒ„
        midi_dir = output_path / "midi_queries"
        midi_dir.mkdir(exist_ok=True)
        
        # æƒææª”æ¡ˆä¸¦æŒ‰TRACKåˆ†çµ„
        files_by_track = self.scan_files_in_range(max_category)
        
        if len(files_by_track) == 0:
            print("éŒ¯èª¤ï¼šæ²’æœ‰æ‰¾åˆ°ä»»ä½•MIDIæª”æ¡ˆ")
            return
        
        # å°‡æ‰€æœ‰æª”æ¡ˆå±•é–‹æˆåˆ—è¡¨ä»¥ä¾¿éš¨æ©Ÿé¸æ“‡
        all_files = []
        for track_id, files in files_by_track.items():
            for file_path, file_name, category in files:
                all_files.append((file_path, track_id, file_name, category))
        
        if len(all_files) < num_queries:
            print(f"è­¦å‘Šï¼šæ‰¾åˆ°çš„æª”æ¡ˆæ•¸é‡ ({len(all_files)}) å°‘æ–¼è¦æ±‚çš„æŸ¥è©¢æ•¸é‡ ({num_queries})")
            num_queries = len(all_files)
        
        # éš¨æ©Ÿé¸æ“‡æª”æ¡ˆ
        selected_files = random.sample(all_files, num_queries)
        
        queries_info = []
        ground_truth_data = []
        successful_queries = 0
        
        print(f"\né–‹å§‹è™•ç†æª”æ¡ˆ...")
        
        for i, (file_path, track_id, file_name, category) in enumerate(selected_files):
            print(f"è™•ç† {i+1}/{num_queries}: {track_id}/{file_name}")
            
            # æå–æŸ¥è©¢ç‰‡æ®µ
            query_info = self.extract_query_segment(file_path, max_duration_seconds)
            
            if query_info is None:
                print(f"  è·³éï¼šç„¡æ³•æå–æœ‰æ•ˆç‰‡æ®µ")
                continue
            
            # ç”ŸæˆæŸ¥è©¢ID
            query_id = f"query_{i+1:04d}_{track_id}"
            
            # å‰µå»ºæŸ¥è©¢MIDIæª”æ¡ˆ
            query_midi_path = midi_dir / f"{query_id}.mid"
            self.create_query_midi(query_info['events'], query_midi_path)
            
            # å»ºç«‹GROUND TRUTH
            query_track_info = {
                'track_id': track_id,
                'file_path': file_path,
                'file_name': file_name,
                'category': category
            }
            ground_truth = self.build_ground_truth(files_by_track, query_track_info)
            
            # è¨˜éŒ„æŸ¥è©¢è³‡è¨Š
            query_record = {
                'query_id': query_id,
                'source_file': file_path,
                'source_track': track_id,
                'source_filename': file_name,
                'category': category,
                'query_midi_path': str(query_midi_path),
                'events': query_info['events'],
                'duration_seconds': query_info['duration_seconds'],
                'num_events': query_info['num_events'],
                'source_start_time': query_info['start_time'],
                'source_end_time': query_info['end_time'],
                'original_duration': query_info['original_duration']
            }
            
            # è¨˜éŒ„GROUND TRUTH
            ground_truth_record = {
                'query_id': query_id,
                'ground_truth': ground_truth
            }
            
            queries_info.append(query_record)
            ground_truth_data.append(ground_truth_record)
            successful_queries += 1
            
            print(f"  æˆåŠŸï¼š{query_info['num_events']} å€‹äº‹ä»¶ï¼Œ{query_info['duration_seconds']:.2f} ç§’")
            print(f"  GTï¼š{ground_truth['total_relevant_docs']} å€‹ç›¸é—œæ–‡æª”")
        
        # ä¿å­˜æŸ¥è©¢è³‡è¨Š
        metadata_file = output_path / "queries_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump({
                'dataset_info': {
                    'generation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'max_category': max_category,
                    'requested_queries': num_queries,
                    'successful_queries': successful_queries,
                    'max_duration_seconds': max_duration_seconds,
                    'total_files_scanned': len(all_files),
                    'total_tracks': len(files_by_track)
                },
                'queries': queries_info
            }, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜GROUND TRUTH
        ground_truth_file = output_path / "ground_truth.json"
        with open(ground_truth_file, 'w', encoding='utf-8') as f:
            json.dump({
                'dataset_info': {
                    'generation_time': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'total_queries': successful_queries,
                    'ground_truth_structure': {
                        'primary_answer': 'Exact source file (relevance_score = 1.0)',
                        'related_answers': 'Other files in same track (relevance_score = 0.8)',
                        'evaluation_note': 'For evaluation, primary_answer should be ranked #1'
                    }
                },
                'ground_truth': ground_truth_data
            }, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆè©•ä¼°ç”¨çš„ç°¡åŒ–GROUND TRUTH
        self.generate_evaluation_ground_truth(ground_truth_data, output_path)
        
        # ç”Ÿæˆçµ±è¨ˆå ±å‘Š
        self.generate_statistics_report(queries_info, ground_truth_data, output_path, files_by_track)
        
        print(f"\næ•¸æ“šé›†ç”Ÿæˆå®Œæˆï¼")
        print(f"æˆåŠŸç”Ÿæˆ {successful_queries} å€‹æŸ¥è©¢")
        print(f"è¼¸å‡ºä½ç½®ï¼š{output_path}")
        print(f"MIDIæª”æ¡ˆï¼š{midi_dir}")
        print(f"æŸ¥è©¢å…ƒæ•¸æ“šï¼š{metadata_file}")
        print(f"GROUND TRUTHï¼š{ground_truth_file}")
        
        return output_path
    
    def generate_evaluation_ground_truth(self, ground_truth_data, output_path):
        """ç”Ÿæˆè©•ä¼°ç”¨çš„ç°¡åŒ–GROUND TRUTHæ ¼å¼"""
        eval_gt = {}
        
        for gt_record in ground_truth_data:
            query_id = gt_record['query_id']
            gt = gt_record['ground_truth']
            
            # ç°¡åŒ–æ ¼å¼ï¼šæŸ¥è©¢ID -> ç›¸é—œæ–‡æª”åˆ—è¡¨ï¼ˆæŒ‰ç›¸é—œæ€§æ’åºï¼‰
            relevant_docs = []
            
            # æ·»åŠ ä¸»è¦ç­”æ¡ˆ
            relevant_docs.append({
                'doc_id': f"{gt['primary_answer']['track_id']}/{gt['primary_answer']['file_name']}",
                'relevance_score': gt['primary_answer']['relevance_score']
            })
            
            # æ·»åŠ ç›¸é—œç­”æ¡ˆ
            for related in gt['related_answers']:
                relevant_docs.append({
                    'doc_id': f"{related['track_id']}/{related['file_name']}",
                    'relevance_score': related['relevance_score']
                })
            
            eval_gt[query_id] = relevant_docs
        
        # ä¿å­˜è©•ä¼°ç”¨GROUND TRUTH
        eval_gt_file = output_path / "evaluation_ground_truth.json"
        with open(eval_gt_file, 'w', encoding='utf-8') as f:
            json.dump({
                'description': 'Simplified ground truth for evaluation metrics (Precision, Recall, MAP)',
                'format': 'query_id -> list of relevant documents with scores',
                'usage': 'Compare retrieval results against relevant_docs list',
                'ground_truth': eval_gt
            }, f, indent=2, ensure_ascii=False)
        
        print(f"è©•ä¼°ç”¨GROUND TRUTHï¼š{eval_gt_file}")
    
    def generate_statistics_report(self, queries_info, ground_truth_data, output_path, files_by_track):
        """ç”Ÿæˆçµ±è¨ˆå ±å‘Šï¼ŒåŒ…å«GROUND TRUTHçµ±è¨ˆ"""
        if not queries_info:
            return
        
        # æ”¶é›†çµ±è¨ˆè³‡è¨Š
        durations = [q['duration_seconds'] for q in queries_info]
        event_counts = [q['num_events'] for q in queries_info]
        categories = [q['category'] for q in queries_info]
        
        # GROUND TRUTHçµ±è¨ˆ
        total_relevant_docs = [gt['ground_truth']['total_relevant_docs'] for gt in ground_truth_data]
        tracks_with_multiple_files = sum(1 for track_files in files_by_track.values() if len(track_files) > 1)
        
        # è½‰æ›NumPyæ•¸æ“šé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹
        def convert_numpy_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        # è™•ç†category_distributionçš„é¡å‹è½‰æ›
        unique_categories, category_counts = np.unique(categories, return_counts=True)
        category_distribution = {str(cat): int(count) for cat, count in zip(unique_categories, category_counts)}
        
        # è™•ç†total_relevant_docs distributionçš„é¡å‹è½‰æ›
        unique_docs, doc_counts = np.unique(total_relevant_docs, return_counts=True)
        doc_distribution = {str(docs): int(count) for docs, count in zip(unique_docs, doc_counts)}
        
        stats = {
            'query_stats': {
                'total_queries': len(queries_info),
                'duration_stats': {
                    'mean': float(np.mean(durations)),
                    'std': float(np.std(durations)),
                    'min': float(np.min(durations)),
                    'max': float(np.max(durations)),
                    'median': float(np.median(durations))
                },
                'event_count_stats': {
                    'mean': float(np.mean(event_counts)),
                    'std': float(np.std(event_counts)),
                    'min': int(np.min(event_counts)),
                    'max': int(np.max(event_counts)),
                    'median': float(np.median(event_counts))
                },
                'category_distribution': category_distribution
            },
            'ground_truth_stats': {
                'total_relevant_docs': {
                    'mean': float(np.mean(total_relevant_docs)),
                    'std': float(np.std(total_relevant_docs)),
                    'min': int(np.min(total_relevant_docs)),
                    'max': int(np.max(total_relevant_docs)),
                    'distribution': doc_distribution
                },
                'tracks_with_multiple_files': int(tracks_with_multiple_files),
                'total_tracks': len(files_by_track),
                'multi_file_ratio': float(tracks_with_multiple_files / len(files_by_track))
            }
        }
        
        # ä¿å­˜çµ±è¨ˆå ±å‘Š
        stats_file = output_path / "statistics_report.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è®€çš„çµ±è¨ˆå ±å‘Š
        report_file = output_path / "dataset_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("æŸ¥è©¢æ•¸æ“šé›†çµ±è¨ˆå ±å‘Šï¼ˆå«GROUND TRUTHï¼‰\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("ğŸ“Š æŸ¥è©¢çµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            f.write(f"ç¸½æŸ¥è©¢æ•¸é‡: {stats['query_stats']['total_queries']}\n\n")
            
            f.write("æ™‚é•·çµ±è¨ˆ (ç§’):\n")
            f.write(f"  å¹³å‡: {stats['query_stats']['duration_stats']['mean']:.2f}\n")
            f.write(f"  æ¨™æº–å·®: {stats['query_stats']['duration_stats']['std']:.2f}\n")
            f.write(f"  æœ€çŸ­: {stats['query_stats']['duration_stats']['min']:.2f}\n")
            f.write(f"  æœ€é•·: {stats['query_stats']['duration_stats']['max']:.2f}\n")
            f.write(f"  ä¸­ä½æ•¸: {stats['query_stats']['duration_stats']['median']:.2f}\n\n")
            
            f.write("äº‹ä»¶æ•¸é‡çµ±è¨ˆ:\n")
            f.write(f"  å¹³å‡: {stats['query_stats']['event_count_stats']['mean']:.1f}\n")
            f.write(f"  æ¨™æº–å·®: {stats['query_stats']['event_count_stats']['std']:.1f}\n")
            f.write(f"  æœ€å°‘: {stats['query_stats']['event_count_stats']['min']}\n")
            f.write(f"  æœ€å¤š: {stats['query_stats']['event_count_stats']['max']}\n")
            f.write(f"  ä¸­ä½æ•¸: {stats['query_stats']['event_count_stats']['median']:.1f}\n\n")
            
            f.write("ğŸ¯ GROUND TRUTHçµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            f.write(f"æ¯å€‹æŸ¥è©¢çš„ç›¸é—œæ–‡æª”æ•¸:\n")
            f.write(f"  å¹³å‡: {stats['ground_truth_stats']['total_relevant_docs']['mean']:.1f}\n")
            f.write(f"  æœ€å°‘: {stats['ground_truth_stats']['total_relevant_docs']['min']}\n")
            f.write(f"  æœ€å¤š: {stats['ground_truth_stats']['total_relevant_docs']['max']}\n\n")
            
            f.write("ç›¸é—œæ–‡æª”æ•¸åˆ†å¸ƒ:\n")
            for doc_count, query_count in sorted(stats['ground_truth_stats']['total_relevant_docs']['distribution'].items()):
                f.write(f"  {doc_count} å€‹ç›¸é—œæ–‡æª”: {query_count} å€‹æŸ¥è©¢\n")
            
            f.write(f"\nTRACKçµ±è¨ˆ:\n")
            f.write(f"  ç¸½TRACKæ•¸: {stats['ground_truth_stats']['total_tracks']}\n")
            f.write(f"  æœ‰å¤šå€‹æª”æ¡ˆçš„TRACK: {stats['ground_truth_stats']['tracks_with_multiple_files']}\n")
            f.write(f"  å¤šæª”æ¡ˆæ¯”ä¾‹: {stats['ground_truth_stats']['multi_file_ratio']:.1%}\n\n")
            
            f.write("ğŸ“ åˆ†é¡åˆ†å¸ƒ\n")
            f.write("-" * 30 + "\n")
            for category, count in sorted(stats['query_stats']['category_distribution'].items()):
                f.write(f"  {category}: {count} å€‹æŸ¥è©¢\n")
            
            f.write(f"\nğŸ’¡ è©•ä¼°å»ºè­°\n")
            f.write("-" * 30 + "\n")
            f.write("â€¢ ä½¿ç”¨ evaluation_ground_truth.json é€²è¡Œæª¢ç´¢è©•ä¼°\n")
            f.write("â€¢ ä¸»è¦ç­”æ¡ˆï¼ˆä¾†æºæª”æ¡ˆï¼‰æ‡‰è©²æ’åœ¨ç¬¬1ä½\n")
            f.write("â€¢ åŒTRACKçš„å…¶ä»–æª”æ¡ˆä¹Ÿç®—ç›¸é—œç­”æ¡ˆ\n")
            f.write("â€¢ å¯è¨ˆç®— Precision@K, Recall@K, MAP ç­‰æŒ‡æ¨™\n")
        
        print(f"çµ±è¨ˆå ±å‘Šå·²ä¿å­˜ï¼š{report_file}")


def main():
    parser = argparse.ArgumentParser(
        description="PPR4ENVæŸ¥è©¢æ•¸æ“šé›†ç”Ÿæˆå™¨ï¼ˆå«GROUND TRUTHï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
    # ç”Ÿæˆ100å€‹æŸ¥è©¢ï¼Œç¯„åœåˆ°CCCï¼Œæœ€é•·10ç§’
    python query_dataset_generator.py --data-path /path/to/lmd_matched --max-category CCC --num-queries 100 --max-duration 10
    
    # ç”Ÿæˆ50å€‹æŸ¥è©¢ï¼Œç¯„åœåˆ°ABCï¼Œæœ€é•·5ç§’
    python query_dataset_generator.py --data-path /path/to/lmd_matched --max-category ABC --num-queries 50 --max-duration 5

è¼¸å‡ºæª”æ¡ˆèªªæ˜:
    â€¢ midi_queries/          - æŸ¥è©¢MIDIæª”æ¡ˆ
    â€¢ queries_metadata.json  - æŸ¥è©¢è©³ç´°è³‡è¨Š
    â€¢ ground_truth.json      - å®Œæ•´GROUND TRUTH
    â€¢ evaluation_ground_truth.json - è©•ä¼°ç”¨ç°¡åŒ–æ ¼å¼
    â€¢ dataset_report.txt     - å¯è®€çµ±è¨ˆå ±å‘Š
        """
    )
    
    parser.add_argument('--data-path', required=True,
                       help='lmd_matchedè³‡æ–™å¤¾è·¯å¾‘')
    parser.add_argument('--max-category', default='CCC',
                       help='æœ€å¤§åˆ†é¡ç¯„åœï¼ˆä¸‰å€‹å­—æ¯ï¼Œä¾‹å¦‚ï¼šABC, CCC, ZZZï¼‰')
    parser.add_argument('--num-queries', type=int, default=100,
                       help='è¦ç”Ÿæˆçš„æŸ¥è©¢æ•¸é‡')
    parser.add_argument('--max-duration', type=float, default=10.0,
                       help='æ¯å€‹æŸ¥è©¢çš„æœ€å¤§æ™‚é•·ï¼ˆç§’ï¼‰')
    parser.add_argument('--output-dir', default='query_dataset',
                       help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--seed', type=int,
                       help='éš¨æ©Ÿç¨®å­ï¼ˆç”¨æ–¼å¯é‡ç¾çš„çµæœï¼‰')
    
    args = parser.parse_args()
    
    # è¨­å®šéš¨æ©Ÿç¨®å­
    if args.seed:
        random.seed(args.seed)
        np.random.seed(args.seed)
        print(f"ä½¿ç”¨éš¨æ©Ÿç¨®å­: {args.seed}")
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = QueryDatasetGenerator(args.data_path)
        
        # ç”Ÿæˆæ•¸æ“šé›†
        output_path = generator.generate_dataset(
            max_category=args.max_category,
            num_queries=args.num_queries,
            max_duration_seconds=args.max_duration,
            output_dir=args.output_dir
        )
        
        print(f"\nâœ… æ•¸æ“šé›†ç”Ÿæˆå®Œæˆï¼è¼¸å‡ºä½ç½®ï¼š{output_path}")
        print(f"\nğŸ“‹ è¼¸å‡ºæª”æ¡ˆï¼š")
        print(f"  â€¢ midi_queries/ - æŸ¥è©¢MIDIæª”æ¡ˆ")
        print(f"  â€¢ queries_metadata.json - æŸ¥è©¢è©³ç´°è³‡è¨Š")
        print(f"  â€¢ ground_truth.json - å®Œæ•´GROUND TRUTH")
        print(f"  â€¢ evaluation_ground_truth.json - è©•ä¼°ç”¨æ ¼å¼")
        print(f"  â€¢ dataset_report.txt - çµ±è¨ˆå ±å‘Š")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()